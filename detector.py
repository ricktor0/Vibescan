import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

# Known AI tool generator strings
AI_GENERATORS = [
    "v0.dev", "bolt.new", "lovable", "cursor", "replit", "copilot",
    "chatgpt", "claude", "gemini", "windsurf", "codeium"
]

# Hosting platforms commonly used by vibe coders
VIBE_HOSTING_HEADERS = {
    "x-vercel-id": ("Vercel hosting detected", 15),
    "x-netlify":   ("Netlify hosting detected", 15),
    "x-railway":   ("Railway hosting detected", 15),
}

VIBE_SERVER_NAMES = ["vercel", "netlify", "railway", "replit"]

# shadcn/ui and common AI-scaffold patterns
SHADCN_PATTERNS = [
    r'data-slot=["\']',
    r'\bcn\(',
    r'"use client"',
    r'@radix-ui',
    r'lucide-react',
    r'class-variance-authority',
    r'clsx',
    r'tailwind-merge',
    r'bg-destructive',
    r'text-accent-foreground',
    r'focus-visible:ring',
    r'hover:bg-accent',
]

# Placeholder text patterns
PLACEHOLDER_PATTERNS = [
    r'Your Company',
    r'your@email\.com',
    r'user@example\.com',
    r'John Doe',
    r'Lorem ipsum',
    r'placeholder text',
    r'Coming soon',
    r'Built with v0',
    r'Generated by',
]

# Next.js / Vite / common AI scaffold signals
FRAMEWORK_PATTERNS = [
    (r'__NEXT_DATA__',              "Next.js (Pages Router) detected",     10),
    (r'/_next/static',              "Next.js static assets detected",       8),
    (r'/@vite/client',              "Vite dev server detected",             8),
    (r'type="module".*\.js',        "ES module bundler (Vite/Rollup)",      5),
    (r'<script[^>]+src=["\'][^"\']*/_next/', "Next.js bundle detected",    8),
]

TAILWIND_PATTERNS = [
    r'cdn\.tailwindcss\.com',
    r'tailwindcss\.com/cdn',
    r'class="[^"]*(?:flex|grid|text-\w+|bg-\w+|p-\d|m-\d)[^"]*"',
]

# --- AI writing vocabulary ---
# These are individual words/short phrases that are statistically overrepresented
# in AI-generated marketing/landing page copy. We score by COUNT of matches,
# not by requiring specific multi-word phrases.
AI_VOCAB = [
    # Action verbs AI loves
    "empower", "leverage", "revolutionize", "transform", "unlock",
    "streamline", "harness", "foster", "accelerate", "elevate",
    "optimize", "innovate", "collaborate", "inspire", "enable",
    "drive", "shape", "build", "create", "connect", "grow",
    "learn", "compete", "explore", "discover", "achieve",
    # Abstract nouns AI loves
    "innovation", "excellence", "potential", "journey", "future",
    "community", "collaboration", "growth", "impact", "success",
    "opportunity", "experience", "solution", "platform", "ecosystem",
    "vision", "mission", "passion", "dedication", "commitment",
    "knowledge", "skills", "expertise", "insights", "outcomes",
    # Adjectives AI loves
    "cutting-edge", "innovative", "seamless", "robust", "scalable",
    "comprehensive", "dynamic", "vibrant", "diverse", "inclusive",
    "meaningful", "practical", "hands-on", "industry-leading",
    "next-generation", "state-of-the-art", "user-friendly",
    # Common AI phrases (treated as single tokens)
    "digital future", "industry professionals", "career paths",
    "skill building", "real-world", "best practices",
    "latest trends", "latest technologies", "stay ahead",
    "make a difference", "take your", "join us", "be part of",
    "learn more", "get started", "sign up", "contact us",
]


def _fetch(url, timeout=8):
    try:
        r = requests.get(url, timeout=timeout, allow_redirects=True,
                         headers={"User-Agent": "Mozilla/5.0 (VibeScan/1.0)"},
                         verify=True)
        return r
    except requests.exceptions.SSLError:
        try:
            return requests.get(url, timeout=timeout, allow_redirects=True,
                                headers={"User-Agent": "Mozilla/5.0 (VibeScan/1.0)"},
                                verify=False)
        except Exception:
            return None
    except Exception:
        return None


def _extract_readable_strings(text: str, min_len: int = 15) -> str:
    """
    Extract human-readable strings from minified JS/JSON.
    Looks for quoted strings that contain mostly letters and spaces.
    """
    # Find all single and double quoted strings
    candidates = re.findall(r'"([^"\\]{' + str(min_len) + r',})"', text)
    candidates += re.findall(r"'([^'\\]{" + str(min_len) + r",})'", text)
    # Keep only strings that look like natural language (>50% letters+spaces)
    readable = []
    for c in candidates:
        letters = sum(1 for ch in c if ch.isalpha() or ch == ' ')
        if letters / max(len(c), 1) > 0.6:
            readable.append(c)
    return ' '.join(readable)


def _fetch_js_chunks(html: str, base_url: str, max_chunks: int = 8) -> str:
    """Fetch Next.js/Vite JS chunks in parallel and return combined text."""
    script_srcs = re.findall(r'<script[^>]+src=["\']([^"\']+)["\']', html)

    # Prioritise page/app/chunk files over polyfills/webpack runtime
    priority, rest = [], []
    for src in script_srcs:
        lower = src.lower()
        if any(k in lower for k in ['page', 'app-', 'app/', 'chunk', 'main', 'index']):
            priority.append(src)
        elif 'polyfill' not in lower and 'webpack' not in lower:
            rest.append(src)

    selected = (priority + rest)[:max_chunks]

    def fetch_one(src):
        url = src if src.startswith("http") else urljoin(base_url, src)
        r = _fetch(url, timeout=6)
        if r and r.status_code == 200 and len(r.text) > 100:
            return r.text
        return ""

    js_texts = []
    with ThreadPoolExecutor(max_workers=8) as ex:
        futures = [ex.submit(fetch_one, src) for src in selected]
        for fut in as_completed(futures, timeout=20):
            try:
                t = fut.result()
                if t:
                    js_texts.append(t)
            except Exception:
                pass

    return "\n".join(js_texts)


def _score_ai_vocab(text: str) -> tuple[int, list[str]]:
    """
    Count how many AI vocabulary words/phrases appear in the text.
    Returns (hit_count, list_of_matched_terms).
    """
    hits = []
    text_lower = text.lower()
    for term in AI_VOCAB:
        # Use word boundary for single words, substring for phrases
        if ' ' in term:
            if term in text_lower:
                hits.append(term)
        else:
            if re.search(r'\b' + re.escape(term) + r'\b', text_lower):
                hits.append(term)
    return len(hits), hits


def detect(url: str, timeout: int = 10) -> dict:
    result = {
        "url": url,
        "score": 0,
        "signals": [],
        "verdict": "",
        "error": None,
    }

    resp = _fetch(url, timeout=timeout)
    if resp is None:
        result["error"] = "Connection refused or host unreachable"
        return result

    html = resp.text
    headers = {k.lower(): v.lower() for k, v in resp.headers.items()}
    soup = BeautifulSoup(html, "html.parser")
    base_url = f"{resp.url.split('//')[0]}//{resp.url.split('//')[1].split('/')[0]}"

    def add_signal(label: str, points: int):
        result["signals"].append({"label": label, "points": points})
        result["score"] += points

    # --- 1. Meta generator tag ---
    meta_gen = soup.find("meta", attrs={"name": "generator"})
    if meta_gen:
        content = (meta_gen.get("content") or "").lower()
        for tool in AI_GENERATORS:
            if tool in content:
                add_signal(f'Meta generator tag: "{meta_gen.get("content")}"', 25)
                break

    # --- 2. HTML comments mentioning AI tools ---
    for comment in re.findall(r'<!--(.*?)-->', html, re.DOTALL):
        for tool in AI_GENERATORS:
            if tool in comment.lower():
                add_signal(f'HTML comment references AI tool: "{tool}"', 20)
                break

    # --- 3. Hosting fingerprint ---
    for header, (label, pts) in VIBE_HOSTING_HEADERS.items():
        if header in headers:
            add_signal(label, pts)

    server = headers.get("server", "")
    for name in VIBE_SERVER_NAMES:
        if name in server:
            add_signal(f'Server header indicates vibe hosting: "{headers.get("server")}"', 15)
            break

    # --- 4. Framework signals ---
    for pattern, label, pts in FRAMEWORK_PATTERNS:
        if re.search(pattern, html):
            add_signal(label, pts)
            break

    # --- 5. Tailwind CSS ---
    if sum(1 for p in TAILWIND_PATTERNS if re.search(p, html)) >= 2:
        add_signal("Tailwind CSS detected (CDN or heavy class usage)", 10)

    # --- 6. Missing security headers ---
    security_headers = ["content-security-policy", "x-frame-options", "strict-transport-security"]
    missing = [h for h in security_headers if h not in headers]
    if len(missing) >= 2:
        add_signal(f"Missing security headers: {', '.join(missing)}", 5)

    # --- 7. Open CORS ---
    if headers.get("access-control-allow-origin") == "*":
        add_signal("Open CORS: Access-Control-Allow-Origin: *", 5)

    # --- 8. Fetch JS chunks ---
    print("  [*] Fetching JS bundles for deep analysis...")
    js_content = _fetch_js_chunks(html, base_url, max_chunks=8)

    # Extract Next.js RSC inline payloads (__next_f) â€” App Router page text
    rsc_payloads = re.findall(r'self\.__next_f\.push\(\[\d+,(.+?)\]\)', html, re.DOTALL)
    rsc_text = ' '.join(rsc_payloads)

    # Build combined corpus: HTML + JS chunks + RSC payloads
    combined_raw = html + "\n" + js_content + "\n" + rsc_text

    # Extract readable strings from JS (strips minified code noise)
    readable_from_js = _extract_readable_strings(js_content + "\n" + rsc_text)

    # Full search corpus: raw combined + extracted readable strings
    search_corpus = combined_raw + "\n" + readable_from_js

    # --- 9. shadcn/ui patterns ---
    shadcn_hits = sum(1 for p in SHADCN_PATTERNS if re.search(p, search_corpus))
    if shadcn_hits >= 4:
        add_signal(f"shadcn/ui component library detected ({shadcn_hits} pattern matches)", 15)
    elif shadcn_hits >= 2:
        add_signal(f"shadcn/ui patterns detected ({shadcn_hits} matches)", 10)
    elif shadcn_hits == 1:
        add_signal("Possible shadcn/ui pattern (1 match)", 5)

    # --- 10. Placeholder text ---
    placeholder_hits = [p for p in PLACEHOLDER_PATTERNS
                        if re.search(p, search_corpus, re.IGNORECASE)]
    if placeholder_hits:
        add_signal(f"Generic placeholder text found ({len(placeholder_hits)} patterns)",
                   min(len(placeholder_hits) * 5, 15))

    # --- 11. AI vocabulary scoring ---
    hit_count, matched_terms = _score_ai_vocab(search_corpus)

    if hit_count >= 15:
        pts = 20
        label = f"Strong AI writing vocabulary ({hit_count} terms: {', '.join(matched_terms[:5])}...)"
    elif hit_count >= 8:
        pts = 14
        label = f"AI writing vocabulary detected ({hit_count} terms: {', '.join(matched_terms[:4])})"
    elif hit_count >= 4:
        pts = 8
        label = f"Possible AI writing style ({hit_count} terms: {', '.join(matched_terms)})"
    elif hit_count >= 1:
        pts = 3
        label = f"Mild AI vocabulary signals ({hit_count} terms: {', '.join(matched_terms)})"
    else:
        pts = 0
        label = None

    if label:
        add_signal(label, pts)

    # --- Cap score at 100 ---
    result["score"] = min(result["score"], 100)

    # --- Verdict ---
    score = result["score"]
    if score <= 20:
        result["verdict"] = "Unlikely vibe coded"
    elif score <= 45:
        result["verdict"] = "Possibly vibe coded"
    elif score <= 70:
        result["verdict"] = "Likely vibe coded"
    else:
        result["verdict"] = "Almost certainly vibe coded"

    return result
